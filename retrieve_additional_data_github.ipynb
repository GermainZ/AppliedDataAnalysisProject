{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook handles the retrieval and creation of two related datasets using the Github API, in order to enrich our main dataset:**\n",
    "\n",
    "- **Github issues**: the issues and pull requests for each of the projects we're interested in.\n",
    "- **Github comments**: for each issue/pull request, its corresponding comments (replies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import pandas as pd\n",
    "from github import Github\n",
    "from datetime import datetime\n",
    "import time\n",
    "from github.GithubException import RateLimitExceededException\n",
    "\n",
    "# Paths.\n",
    "DATA_DIR = './data'\n",
    "DATASETS_DIR = './data/datasets'\n",
    "OAUTH_TOKEN = 'secret'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Github API client.\n",
    "g = Github(OAUTH_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fetch each repo's issues and comments, we first need to determine the Github repository each directory in our Gitential dataset belongs to.\n",
    "\n",
    "The directory structure is as follows:\n",
    "```\n",
    "data/\n",
    "    datasets/\n",
    "        angular-angular/\n",
    "        antirez-redis/\n",
    "        apache-incubator-superset/\n",
    "        apache-mesos/\n",
    "        keras-team-keras/\n",
    "        pandas-dev-pandas/\n",
    "        ...\n",
    "```\n",
    "\n",
    "For example, `angular-angular` corresponds to the Github project `angular/angular` (owner: angular, project name: angular).\n",
    "\n",
    "However, some of these names are ambiguous. For instance, does `apache-incubator-superset` correspond to `apache-incubator/superset` or `apache/incubator-superset`? We handle these few cases manually, and resolve the rest automatically by replacing `-` with `/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ambiguous names and their corresponding Github repos.\n",
    "AMBIGUOUS_NAMES = {\n",
    "    'apache-incubator-superset': 'apache-incubator/superset',\n",
    "    'keras-team-keras': 'keras-team/keras',\n",
    "    'pandas-dev-pandas': 'pandas-dev/pandas',\n",
    "    'rust-lang-rust': 'rust-lang/rust',\n",
    "    'scikit-learn-scikit-learn': 'scikit-learn/scikit-learn'\n",
    "}\n",
    "# This is where we'll store the directory<->Github repo mapping.\n",
    "DIR_GITHUB_MAPPING = {}\n",
    "\n",
    "# Go over the directories.\n",
    "for dir_name in os.listdir(DATASETS_DIR):\n",
    "    if dir_name in AMBIGUOUS_NAMES:\n",
    "        # Handle ambiguous names.\n",
    "        github_path = AMBIGUOUS_NAMES[dir_name]\n",
    "    else:\n",
    "        # Handle non-ambiguous names.\n",
    "        github_path = dir_name.replace('-', '/')\n",
    "    # Update the mapping.\n",
    "    DIR_GITHUB_MAPPING[dir_name] = github_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each Github repository we're interested in, we extract the following data for the **issues**:\n",
    "\n",
    "- **id**: a (global) ID of the issue.\n",
    "- **title**: the title of the issue.\n",
    "- **body**: the body text of the issue.\n",
    "- **number**: the number of the issue (for the current repo).\n",
    "- **comments**: number of comments.\n",
    "- **html_url**: the URL to the issue.\n",
    "- **state**: whether the issue is open or closed.\n",
    "- **user**: the username of the person who opened the issue.\n",
    "- **created_at**: when the issue was created.\n",
    "- **updated_at**: when the issue was last updated.\n",
    "- **closed_at**: when the issue was closed (or `None` if it was never closed).\n",
    "- **closed_by**: the username of the person who closed the issue (`None` if it was never closed).\n",
    "- **assignee**: (legacy) the username of the person who is assigned to this issue  (`None` if no one is assigned).\n",
    "- **assignees**: same as above, but supports multiple users (`None` if there are no assignees).\n",
    "- **labels**: the list of labels applied to this issue (`None` if there are no labels).\n",
    "- **milestone**: the milestone's title that is applied to this issue (`None` if no milestone is applied).\n",
    "- **pull_request**: the pull request URL if this issue is a pull request (`None` if it's a normal issue).\n",
    "- **project**: the project's name for which this issue belongs.\n",
    "\n",
    "For each issue, we also extract the following data for its **comments**:\n",
    "\n",
    "- **body**: the text of the comment.\n",
    "- **created_at**: when the comment was created.\n",
    "- **updated_at**: when the comment was last updated.\n",
    "\n",
    "Note that the Github API has a limit of 5000 requests/hour, which we handle as well in the code by waiting whenever necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we'll store the data we're fetching.\n",
    "issues_data = {}\n",
    "comments_data = {}\n",
    "\n",
    "# Descriptors we want to fetch for issues/comments without any special handling.\n",
    "# Some descriptors require some extra logic, which we do in the main code.\n",
    "descriptors = ['body', 'closed_at', 'comments', 'created_at', 'html_url', 'number', 'state', 'title', 'updated_at']\n",
    "comments_descriptors = ['body', 'created_at', 'updated_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching issues for pandas-dev/pandas\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 00:48:16.554675)... (pandas-dev/pandas)\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 01:47:02.831549)... (pandas-dev/pandas)\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 02:53:55.383979)... (pandas-dev/pandas)\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 04:00:50.323763)... (pandas-dev/pandas)\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 05:07:13.648489)... (pandas-dev/pandas)\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 06:14:07.311023)... (pandas-dev/pandas)\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 07:20:10.565698)... (pandas-dev/pandas)\n",
      "Fetching issues for Microsoft/CNTK\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 08:25:44.606466)... (Microsoft/CNTK)\n",
      "Fetching issues for caffe2/caffe2\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 09:31:49.292134)... (caffe2/caffe2)\n",
      "Fetching issues for apache/spark\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 10:38:45.413574)... (apache/spark)\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 11:49:00.688177)... (apache/spark)\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 12:47:27.487134)... (apache/spark)\n",
      "Rate limit exceeded. Sleeping (time: 2018-11-25 13:56:08.514292)... (apache/spark)\n"
     ]
    }
   ],
   "source": [
    "def extract_issue_data(issues_data, comments_data, issue, github_path):\n",
    "    '''Given an issue, fetch its data and comments and store them in issues_data and comments_data.'''\n",
    "    # Get and store the issue's comments.\n",
    "    for comment in issue.get_comments():\n",
    "        comment_data = {}\n",
    "        for descriptor in comments_descriptors:\n",
    "            comment_data[descriptor] = getattr(comment, descriptor)\n",
    "        comment_data['parent'] = issue.id\n",
    "        comments_data[comment.id] = comment_data\n",
    "    # Get and store the issue's data.\n",
    "    issue_data = {}\n",
    "    for descriptor in descriptors:\n",
    "        issue_data[descriptor] = getattr(issue, descriptor)\n",
    "    issue_data['closed_by'] = issue.closed_by.login if issue.closed_by else None\n",
    "    issue_data['user'] = issue.user.login\n",
    "    issue_data['assignee'] = issue.assignee.login if issue.assignee else None\n",
    "    issue_data['assignees'] = [user.login for user in issue.assignees] if issue.assignees else None\n",
    "    issue_data['labels'] = [label.name for label in issue.labels] if issue.labels else None\n",
    "    issue_data['milestone'] = issue.milestone.title if issue.milestone else None\n",
    "    issue_data['pull_request'] = issue.pull_request.html_url if issue.pull_request else None\n",
    "    issue_data['project'] = github_path\n",
    "    issues_data[issue.id] = issue_data\n",
    "\n",
    "# For each directory and corresponding Github repo, fetch the issues and comments.\n",
    "for dir_name, github_path in DIR_GITHUB_MAPPING.items():\n",
    "    print('Fetching issues for {}'.format(github_path))\n",
    "    repo = g.get_repo(github_path)\n",
    "    for issue in repo.get_issues(state='all'):\n",
    "        # We don't want our code to crash when Github returns an error (e.g. API rate limit exceeded).\n",
    "        # Therefore, we keep trying/sleeping until fetching an issue's data succeeds.\n",
    "        success = False\n",
    "        while not success:\n",
    "            try:\n",
    "                # If we've already retrieved this issue's data, skip it.\n",
    "                if issue.id not in issues_data:\n",
    "                    extract_issue_data(issues_data, comments_data, issue, github_path)\n",
    "                success = True\n",
    "            except RateLimitExceededException as exc:\n",
    "                # If we've exceeded the rate limit, sleep and try again later.\n",
    "                # While the limit is hourly, we sleep for 10 minutes at a time instead of 1 hour at a time\n",
    "                # because the limit refresh seems to lag a bit sometimes. This way, we don't risk waiting an\n",
    "                # extra hour if it lagged a bit too much.\n",
    "                print('Rate limit exceeded. Sleeping (time: {})... ({})'.format(datetime.now(), github_path))\n",
    "                remaining = 0\n",
    "                while remaining < 4000:\n",
    "                    time.sleep(10 * 60)\n",
    "                    remaining = g.get_rate_limit().core.remaining\n",
    "            except GithubException as exc:\n",
    "                print(exc)\n",
    "                time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our dictionaries into pandas DataFrames for easier handling.\n",
    "issues_df = pd.DataFrame.from_dict(issues_data, orient='index')\n",
    "comments_df = pd.DataFrame.from_dict(comments_data, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that the dataset is still retrieving. The code below was only performed on a sample, which will further be explored in the main notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas-dev/pandas    1963\n",
       "Name: html_url, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of retrieved issues from each project.\n",
    "issues_df.html_url.str.split('/').apply(lambda x: x[3] + '/' + x[4]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk.\n",
    "comments_df.to_csv('{}/github_comments.csv'.format(DATA_DIR))\n",
    "issues_df.to_csv('{}/github_issues.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
